import os
import gymnasium as gym
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from metadrive import MetaDriveEnv

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL_TIMESTEPS = 1_000_000
SAVE_FREQ       = 100_000
LOG_DIR         = "./logs/"
MODEL_DIR       = "./models/"
MODEL_NAME      = "ppo_metadrive_phase1"

os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

def make_env():
    def _init():
        env = MetaDriveEnv(config={
            "use_render":              True,   # headless = faster training
            "num_scenarios":           10,      # variety of tracks
            "traffic_density":         0.0,     # no traffic in phase 1
            "accident_prob":           0.0,
            "map":                     "SSSSSSSS",  # simple straight roads
            "random_lane_width":       False,
            "random_agent_model":      False,
            # "vehicle_config": {
            #     "use_saver":           False,
            # },
        })
        env = Monitor(env, LOG_DIR)  # wraps env to log episode stats
        return env
    return _init

def inspect_reward():
    print("\nðŸ” Inspecting reward signal for 1 episode...\n")
    env = MetaDriveEnv(config={"use_render": False})
    obs, _ = env.reset()
    total_reward = 0
    for step in range(300):
        action = env.action_space.sample()
        obs, reward, terminated, truncated, info = env.step(action)
        total_reward += reward
        if terminated or truncated:
            break
    print(f"  Episode length : {info['episode_length']}")
    print(f"  Total reward   : {total_reward:.4f}")
    print(f"  Route completion: {info['route_completion']:.4f}")
    print(f"  Crash          : {info['crash']}")
    print(f"  Out of road    : {info['out_of_road']}")
    print(f"  Final velocity : {info['velocity']:.4f}")
    print()
    env.close()

def train():
    vec_env = DummyVecEnv([make_env()])
    vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)

    # Eval environment (separate, not normalized by same stats)
    eval_env = DummyVecEnv([make_env()])
    eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, training=False)

    # PPO Agent
    model = PPO(
        policy          = "MlpPolicy",   # vector obs â†’ MLP â†’ actions
        env             = vec_env,
        learning_rate   = 3e-4,
        n_steps         = 2048,          # steps per env before update
        batch_size      = 64,
        n_epochs        = 10,
        gamma           = 0.99,          # discount factor
        gae_lambda      = 0.95,
        clip_range      = 0.2,
        ent_coef        = 0.01,          # encourages exploration
        verbose         = 1,
        tensorboard_log = LOG_DIR,
    )

    # Save a checkpoint every SAVE_FREQ steps
    checkpoint_callback = CheckpointCallback(
        save_freq   = SAVE_FREQ,
        save_path   = MODEL_DIR,
        name_prefix = MODEL_NAME,
        verbose     = 1,
    )

    # Evaluate agent every 50k steps, save best model
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path = os.path.join(MODEL_DIR, "best"),
        log_path             = LOG_DIR,
        eval_freq            = 50_000,
        n_eval_episodes      = 5,
        deterministic        = True,
        verbose              = 1,
    )

    print("ðŸš— Starting Phase 1 Training...\n")
    print(f"   Total timesteps : {TOTAL_TIMESTEPS:,}")
    print(f"   Checkpoints     : every {SAVE_FREQ:,} steps â†’ {MODEL_DIR}")
    print(f"   TensorBoard     : {LOG_DIR}")
    print(f"\n   Run this to monitor:")
    print(f"   tensorboard --logdir {LOG_DIR}\n")

    model.learn(
        total_timesteps = TOTAL_TIMESTEPS,
        callback        = [checkpoint_callback, eval_callback],
        progress_bar    = True,
    )

    # Save final model + normalization stats
    final_path = os.path.join(MODEL_DIR, MODEL_NAME + "_final")
    model.save(final_path)
    vec_env.save(os.path.join(MODEL_DIR, "vecnormalize.pkl"))
    print(f"\nâœ… Training complete. Model saved to: {final_path}")

def evaluate():
    from stable_baselines3.common.vec_env import VecNormalize
    import pickle

    print("\nðŸŽ® Running evaluation with rendering...\n")

    vec_env = DummyVecEnv([lambda: Monitor(MetaDriveEnv(config={
        "use_render":      True,
        "num_scenarios":   10,
        "traffic_density": 0.0,
    }), LOG_DIR)])

    # Load normalization stats from training
    norm_path = os.path.join(MODEL_DIR, "vecnormalize.pkl")
    if os.path.exists(norm_path):
        vec_env = VecNormalize.load(norm_path, vec_env)
        vec_env.training = False
        vec_env.norm_reward = False

    # Load best model
    best_path = os.path.join(MODEL_DIR, "best", "best_model")
    model = PPO.load(best_path, env=vec_env)

    obs = vec_env.reset()
    episode_rewards = []
    total_reward = 0
    episode = 1

    print(f"  Running Episode {episode}...")
    for _ in range(3000):
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, done, info = vec_env.step(action)
        total_reward += reward[0]

        if done[0]:
            episode_rewards.append(total_reward)
            print(f"  Episode {episode} reward: {total_reward:.2f}")
            total_reward = 0
            episode += 1
            obs = vec_env.reset()
            if episode > 5:
                break

    print(f"\n  Average reward over {len(episode_rewards)} episodes: {np.mean(episode_rewards):.2f}")
    vec_env.close()

if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "eval":
        evaluate()
    else:
        inspect_reward()
        train()